{"cells":[{"cell_type":"markdown","metadata":{"id":"fr75-y9A4zcM"},"source":["# Programming Assignment 1: Learning Distributed Word Representations\n","\n","**Due Date**: October 4, 2023 by 2pm\n","\n","\n","**Submission:**\n","You must submit two files:\n","1. [ ] A PDF file containing your writeup, which will be the PDF export of this notebook (i.e., by printing this notebook webpage as PDF). Note that after you generate the PDF file of the Notebook, you will need to **concatenate it with the PDF file of your solutions to written problems**. Make sure that the relevant outputs (e.g. `print_gradients()` outputs, plots, etc.) are included and clearly visible. The **concatenated PDF file** needs to be submitted to **GradeScope**.\n","2. [ ] The `hw1_code_<YOUR_UNI>.ipynb` iPython Notebook, where `<YOUR_UNI>` is your uni. This file needs to be submitted to the **CourseWorks assignment page**.\n","\n","The programming assignments are individual work.  \n","\n","You should attempt all questions for this assignment. Most of them can be answered at least partially even if you were unable to finish earlier questions. If you think your computational results are incorrect, please say so; that may help you get partial credit.\n","   \n","    \n","# Introduction\n","In this assignment we will learn about word embeddings and make neural networks learn about words.\n","We could try to match statistics about the words, or we could train a network that takes a sequence of words as input and learns to predict the word that comes next.\n","        \n","This assignment will ask you to implement a linear embedding and then the backpropagation computations for a neural language model and then run some experiments to analyze the learned representation.\n","The amount of code you have to write is very short but each line will require you to think very carefully.\n","You will need to derive the updates mathematically, and then implement them using matrix and vector operations in NumPy."]},{"cell_type":"markdown","metadata":{"id":"-UUSJPdr3Ge_"},"source":["# Starter code and data\n","\n","First, perform the required imports for your code:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRwuwhoJ3Knl"},"outputs":[],"source":["import collections\n","import pickle\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import pylab\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import sys\n","\n","TINY = 1e-30\n","EPS = 1e-4\n","nax = np.newaxis"]},{"cell_type":"markdown","metadata":{"id":"qNLvRXdy3NDO"},"source":["If you're using colaboratory, this following script creates a folder - here we used 'A1' - in order to download and store the data. If you're not using colaboratory, then set the path to wherever you want the contents to be stored at locally.\n","\n","You can also manually download and unzip the data from [http://www.cs.columbia.edu/~zemel/Class/Nndl/files/a1_data.tar.gz] and put them in the same folder as where you store this notebook.\n","\n","Feel free to use a different way to access the files *data.pk* , *partially_trained.pk*, and *raw_sentences.txt*.\n","\n","The file *raw_sentences.txt* contains the sentences that we will be using for this assignment.\n","These sentences are fairly simple ones and cover a vocabulary of only 250 words (+ 1 special `[MASK]` token word).\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gkug8am63SzY"},"outputs":[],"source":["######################################################################\n","# Setup working directory\n","######################################################################\n","# Change this to a local path if running locally\n","%mkdir -p /content/A1/\n","%cd /content/A1\n","\n","######################################################################\n","# Helper functions for loading data\n","######################################################################\n","# adapted from\n","# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","\n","    print('File path: %s' % fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUQjRpWqnkzk"},"outputs":[],"source":["# Download the dataset and partially pre-trained model\n","get_file(fname='a1_data',\n","                         origin='http://www.cs.columbia.edu/~zemel/Class/Nndl/files/a1_data.tar.gz',\n","                         untar=True)\n","drive_location = 'data'\n","PARTIALLY_TRAINED_MODEL = drive_location + '/' + 'partially_trained.pk'\n","data_location = drive_location + '/' + 'data.pk'"]},{"cell_type":"markdown","metadata":{"id":"Qna9z_wJ3U5e"},"source":["We have already extracted the 4-grams from this dataset and divided them into training, validation, and test sets.\n","To inspect this data, run the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD1LN16d3a0u"},"outputs":[],"source":["data = pickle.load(open(data_location, 'rb'))\n","print(data['vocab'][0]) # First word in vocab is [MASK]\n","print(data['vocab'][1])\n","print(len(data['vocab'])) # Number of words in vocab\n","print(data['vocab']) # All the words in vocab\n","print(data['train_inputs'][:10]) # 10 example training instances"]},{"cell_type":"markdown","metadata":{"id":"lXd2Msqs3fPQ"},"source":["Now `data` is a Python dict which contains the vocabulary, as well as the inputs and targets for all three splits of the data. `data['vocab']` is a list of the 251 words in the dictionary; `data['vocab'][0]` is the word with index 0, and so on. `data['train_inputs']` is a 372,500 x 4 matrix where each row gives the indices of the 4 consecutive context words for one of the 372,500 training cases.\n","The validation and test sets are handled analogously.\n","\n","Even though you only have to modify two specific locations in the code, you may want to read through this code before starting the assignment."]},{"cell_type":"markdown","metadata":{"id":"pa9ggqxJPPs0"},"source":["# Part 1: GLoVE Word Representations (16pts)\n","\n","In this part of the assignment, you will implement a simplified version of the GLoVE embedding (please see the handout for detailed description of the algorithm) with the loss defined as\n","\n","$$L(\\{\\mathbf{w}_i,\\tilde{\\mathbf{w}}_i,b_i, \\tilde{b}_i\\}_{i=1}^V) = \\sum_{i,j=1}^V (\\mathbf{w}_i^\\top\\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$.\n","\n","Note that each word is represented by two $d$-dimensional embedding vectors $\\mathbf{w}_i, \\tilde{\\mathbf{w}}_i$ and two scalar biases $b_i, \\tilde{b}_i$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xo1R6rfP4aJQ"},"source":["Answer the following questions:\n","\n","## 1.1.  GLoVE Parameter Count \\[1pt\\]\n","Given the vocabulary size $V$ and embedding dimensionality $d$, how many parameters does the GLoVE model have?  Note that each word in the vocabulary is associated with 2 embedding vectors and 2 biases."]},{"cell_type":"markdown","metadata":{"id":"gREV4DxJx98K"},"source":["1.1 **Answer**: **\\*\\*TODO: Write Part 1.2 answer here\\*\\***"]},{"cell_type":"markdown","metadata":{"id":"_vQIRZynyGpl"},"source":["## 1.2. Expression for gradient $\\frac{\\partial L}{\\partial \\mathbf{w}_i}$ and $\\frac{\\partial L}{\\partial \\mathbf{b}_i}$ \\[6pts\\]\n","\n","Write the expression for $\\frac{\\partial L}{\\partial \\mathbf{w}_i}$ and $\\frac{\\partial L}{\\partial \\mathbf{b}_i}$, the gradient of the loss function $L$ with respect to parameter vector $\\mathbf{w}_i$ and $\\mathbf{b}_i$. The gradient should be a function of $\\mathbf{w}, \\tilde{\\mathbf{w}}, b, \\tilde{b}, X$ with appropriate subscripts (if any)."]},{"cell_type":"markdown","metadata":{"id":"HYDCmo7UyLyI"},"source":["1.2 **Answer**: **\\*\\*TODO: Write Part 1.2 answer here \\*\\***\n"]},{"cell_type":"markdown","metadata":{"id":"HzWek3lP0p2e"},"source":["## 1.3.  Implement the gradient update of GLoVE. \\[6pts\\]\n","\n","**See** `YOUR CODE HERE` **Comment below for where to complete the code**\n"]},{"cell_type":"markdown","metadata":{"id":"4wTgnxAz0clX"},"source":["We have provided a few functions for training the embedding:\n","\n","*   `calculate_log_co_occurence` computes the log co-occurrence matrix of a given corpus\n","*   `train_GLoVE` runs momentum gradient descent to optimize the embedding\n","*   `loss_GLoVE`:\n","  * INPUT - $V\\times d$ matrix `W` (collection of $V$ embedding vectors, each $d$-dimensional); $V\\times d$ matrix `W_tilde`; $V\\times 1$ vector `b` (collection of $V$ bias terms); $V\\times 1$ vector `b_tilde`; $V \\times V$ log co-occurrence matrix.\n","  * OUTPUT - loss of the GLoVE objective\n","*   `grad_GLoVE`: **TO BE IMPLEMENTED.**\n","  * INPUT:\n","      * $V\\times d$ matrix `W` (collection of $V$ embedding vectors, each $d$-dimensional), embedding for first word;\n","      * $V\\times d$ matrix `W_tilde`, embedding for second word;\n","      * $V\\times 1$ vector `b` (collection of $V$ bias terms);\n","      * $V\\times 1$ vector `b_tilde`, bias for second word;\n","      * $V \\times V$ log co-occurrence matrix.\n","  * OUTPUT:\n","      * $V\\times d$ matrix `grad_W` containing the gradient of the loss function w.r.t. `W`;\n","      * $V\\times d$ matrix `grad_W_tilde` containing the gradient of the loss function w.r.t. `W_tilde`;\n","      * $V\\times 1$ vector `grad_b` which is the gradient of the loss function w.r.t. `b`.\n","      * $V\\times 1$ vector `grad_b_tilde` which is the gradient of the loss function w.r.t. `b_tilde`.\n","\n","Run the code to compute the co-occurence matrix.\n","Make sure to add a 1 to the occurences, so there are no 0's in the matrix when we take the elementwise log of the matrix.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbYUnHGQNbgu"},"outputs":[],"source":["vocab_size = len(data['vocab']) # Number of vocabs\n","\n","def calculate_log_co_occurence(word_data, symmetric=False):\n","  \"Compute the log-co-occurence matrix for our data.\"\n","  log_co_occurence = np.zeros((vocab_size, vocab_size))\n","  for input in word_data:\n","    # Note: the co-occurence matrix may not be symmetric\n","    log_co_occurence[input[0], input[1]] += 1\n","    log_co_occurence[input[1], input[2]] += 1\n","    log_co_occurence[input[2], input[3]] += 1\n","    # If we want symmetric co-occurence can also increment for these.\n","    if symmetric:\n","      log_co_occurence[input[1], input[0]] += 1\n","      log_co_occurence[input[2], input[1]] += 1\n","      log_co_occurence[input[3], input[2]] += 1\n","  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n","  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n","  log_co_occurence = np.log(log_co_occurence)\n","  return log_co_occurence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsIN6RrwONPf"},"outputs":[],"source":["asym_log_co_occurence_train = calculate_log_co_occurence(data['train_inputs'], symmetric=False)\n","asym_log_co_occurence_valid = calculate_log_co_occurence(data['valid_inputs'], symmetric=False)"]},{"cell_type":"markdown","metadata":{"id":"gNnKkMy-d2bB"},"source":["* [ ] **TO BE IMPLEMENTED**: Calculate the gradient of the loss function w.r.t. the parameters $W$, $\\tilde{W}$, $\\mathbf{b}$, and $\\mathbf{b}$. You should vectorize the computation, i.e. not loop over every word. The reading of the matrix cookbook from https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf might be useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbpkXeaAdwnj"},"outputs":[],"source":["def loss_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence):\n","  \"Compute the GLoVE loss.\"\n","  n,_ = log_co_occurence.shape\n","  if W_tilde is None and b_tilde is None:\n","    return np.sum((W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence)**2)\n","  else:\n","    return np.sum((W @ W_tilde.T + b @ np.ones([1,n]) + np.ones([n,1])@b_tilde.T - log_co_occurence)**2)\n","\n","def grad_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence):\n","  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n","  \"INPUT: W - Vxd; W_tilde - Vxd; b - Vx1; b_tilde - Vx1; log_co_occurence: VxV\"\n","  \"OUTPUT: grad_W - Vxd; grad_W_tilde - Vxd, grad_b - Vx1, grad_b_tilde - Vx1\"\n","  n,_ = log_co_occurence.shape\n","\n","  if not W_tilde is None and not b_tilde is None:\n","  ###########################   YOUR CODE HERE  ##############################\n","    pass\n","  ############################################################################\n","  else:\n","    loss = (W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - 0.5*(log_co_occurence + log_co_occurence.T))\n","    grad_W = 4 *(W.T @ loss).T\n","    grad_W_tilde = None\n","    grad_b = 4 * (np.ones([1,n]) @ loss).T\n","    grad_b_tilde = None\n","\n","  return grad_W, grad_W_tilde, grad_b, grad_b_tilde\n","\n","def train_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n","  \"Traing W and b according to GLoVE objective.\"\n","  n,_ = log_co_occurence_train.shape\n","  learning_rate = 0.05 / n  # A hyperparameter.  You can play with this if you want.\n","  for epoch in range(n_epochs):\n","    grad_W, grad_W_tilde, grad_b, grad_b_tilde = grad_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence_train)\n","    W = W - learning_rate * grad_W\n","    b = b - learning_rate * grad_b\n","    if not grad_W_tilde is None and not grad_b_tilde is None:\n","      W_tilde = W_tilde - learning_rate * grad_W_tilde\n","      b_tilde = b_tilde - learning_rate * grad_b_tilde\n","    train_loss, valid_loss = loss_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence_train), loss_GLoVE(W, W_tilde, b, b_tilde, log_co_occurence_valid)\n","    if do_print:\n","      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_W**2)}\")\n","  return W, W_tilde, b, b_tilde, train_loss, valid_loss"]},{"cell_type":"markdown","metadata":{"id":"bRoG_sqZySHD"},"source":["## 1.4. Effect of embedding dimension $d$ \\[3pts\\]\n","Train the both the symmetric and asymmetric GLoVe model with varying dimensionality $d$ by running the cell below. Comment on:\n","1. Which $d$ leads to optimal validation performance for the asymmetric and symmetric models?\n","2. Why does / doesn't larger $d$ always lead to better validation error?\n","3. Which model is performing better, and why?\n"]},{"cell_type":"markdown","metadata":{"id":"mD5jnHJB2hFy"},"source":["1.4 Answer: **\\*\\*TODO: Write Part 1.4 answer here\\*\\***"]},{"cell_type":"markdown","metadata":{"id":"pjiNQ0WkWi1Z"},"source":["Train the GLoVE model for a range of embedding dimensions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46yGUezEMLJe"},"outputs":[],"source":["np.random.seed(1)\n","n_epochs = 500  # A hyperparameter.  You can play with this if you want.\n","embedding_dims = np.array([1, 2, 10, 128, 256])  # Play with this\n","# Store the final losses for graphing\n","asymModel_asymCoOc_final_train_losses, asymModel_asymCoOc_final_val_losses = [], []\n","symModel_asymCoOc_final_train_losses, symModel_asymCoOc_final_val_losses = [], []\n","Asym_W_final_2d, Asym_b_final_2d, Asym_W_tilde_final_2d, Asym_b_tilde_final_2d = None, None, None, None\n","W_final_2d, b_final_2d = None, None\n","do_print = False  # If you want to see diagnostic information during training\n","\n","for embedding_dim in tqdm(embedding_dims):\n","  init_variance = 0.1  # A hyperparameter.  You can play with this if you want.\n","  W = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n","  W_tilde = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n","  b = init_variance * np.random.normal(size=(vocab_size, 1))\n","  b_tilde = init_variance * np.random.normal(size=(vocab_size, 1))\n","  if do_print:\n","    print(f\"Training for embedding dimension: {embedding_dim}\")\n","\n","  # Train Asym model on Asym Co-Oc matrix\n","  Asym_W_final, Asym_W_tilde_final, Asym_b_final, Asym_b_tilde_final, train_loss, valid_loss = train_GLoVE(W, W_tilde, b, b_tilde, asym_log_co_occurence_train, asym_log_co_occurence_valid, n_epochs, do_print=do_print)\n","  if embedding_dim == 2:\n","    # Save a parameter copy if we are training 2d embedding for visualization later\n","    Asym_W_final_2d = Asym_W_final\n","    Asym_W_tilde_final_2d = Asym_W_tilde_final\n","    Asym_b_final_2d = Asym_b_final\n","    Asym_b_tilde_final_2d = Asym_b_tilde_final\n","  asymModel_asymCoOc_final_train_losses += [train_loss]\n","  asymModel_asymCoOc_final_val_losses += [valid_loss]\n","  if do_print:\n","    print(f\"Final validation loss: {valid_loss}\")\n","\n","  # Train Sym model on Asym Co-Oc matrix\n","  W_final, W_tilde_final, b_final, b_tilde_final, train_loss, valid_loss = train_GLoVE(W, None, b, None, asym_log_co_occurence_train, asym_log_co_occurence_valid, n_epochs, do_print=do_print)\n","  if embedding_dim == 2:\n","    # Save a parameter copy if we are training 2d embedding for visualization later\n","    W_final_2d = W_final\n","    b_final_2d = b_final\n","  symModel_asymCoOc_final_train_losses += [train_loss]\n","  symModel_asymCoOc_final_val_losses += [valid_loss]\n","  if do_print:\n","    print(f\"Final validation loss: {valid_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"hzV-qFf5WfAp"},"source":["Plot the training and validation losses against the embedding dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHgHgSzJTg5d"},"outputs":[],"source":["pylab.loglog(embedding_dims, asymModel_asymCoOc_final_train_losses, label=\"Asymmetric Model / Asymmetric Co-Oc\", linestyle=\"--\")\n","pylab.loglog(embedding_dims, symModel_asymCoOc_final_train_losses , label=\"Symmetric Model / Asymmetric Co-Oc\")\n","pylab.xlabel(\"Embedding Dimension\")\n","pylab.ylabel(\"Training Loss\")\n","pylab.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UJSfg_hfvIV"},"outputs":[],"source":["pylab.loglog(embedding_dims, asymModel_asymCoOc_final_val_losses, label=\"Asymmetric Model / Asymmetric Co-Oc\", linestyle=\"--\")\n","pylab.loglog(embedding_dims, symModel_asymCoOc_final_val_losses , label=\"Sym Model / Asymmetric Co-Oc\")\n","pylab.xlabel(\"Embedding Dimension\")\n","pylab.ylabel(\"Validation Loss\")\n","pylab.legend(loc=\"upper left\")"]},{"cell_type":"markdown","metadata":{"id":"C8FZTyrzlCNl"},"source":["# Part 2: Training the model (26pts)\n","\n","We will modify the architecture slightly from the previous section, inspired by BERT \\citep{devlin2018bert}. Instead of having only one output, the architecture will now take in $N=4$ context words, and also output predictions for $N=4$ words. See Figure 2 diagram in the handout for the diagram of this architecture."]},{"cell_type":"markdown","metadata":{"id":"ua0qkOH1tqHw"},"source":["During training, we randomly sample one of the $N$ context words to replace with a `[MASK]` token. The goal is for the network to predict the word that was masked, at the corresponding output word position.\n","In practice, this `[MASK]` token is assigned the index 0 in our dictionary.\n","The weights $W^{(2)}$ = `hid_to_output_weights` now has the shape $NV \\times H$, as the output layer has $NV$ neurons, where the first $V$ output units are for predicting the first word, then the next $V$ are for predicting the second word, and so on.\n","    We call this as *concatenating* output uniits across all word positions, i.e. the $(j + nV)$-th column is for the word $j$ in vocabulary for the $n$-th output word position.\n","        Note here that the softmax is applied in chunks of $V$ as well, to give a valid probability distribution over the $V$ words. Only the output word positions that were masked in the input are included in the cross entropy loss calculation:\n","        \n","        \n","There are three classes defined in this part: `Params`, `Activations`, `Model`.\n","You will make changes to `Model`, but it may help to read through `Params` and `Activations` first.\n","\n","$$C = -\\sum_{i}^{B}\\sum_{n}^{N}\\sum_{j}^{V} m^{(i)}_{n} (t^{(i)}_{n,j} \\log y^{(i)}_{n,j}),$$\n","\n","Where $y^{(i)}_{n,j}$ denotes the output probability prediction from the neural network for the $i$-th training example for the word $j$ in the $n$-th output word, and $t^{(i)}_{n,j}$ is 1 if for the $i$-th training example, the word $j$ is the $n$-th word in context. Finally, $m^{(i)}_{n} \\in \\{0,1\\}$ is a mask that is set to 1 if we are predicting the $n$-th word position for the $i$-th example (because we had masked that word in the input), and 0 otherwise.  \n","\n","\n","There are three classes defined in this part: `Params`, `Activations`, `Model`.\n","You will make changes to `Model`, but it may help to read through `Params` and `Activations` first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfGEjB3QLNXf"},"outputs":[],"source":["class Params(object):\n","    \"\"\"A class representing the trainable parameters of the model. This class has five fields:\n","\n","           word_embedding_weights, a matrix of size V x D, where V is the number of words in the vocabulary\n","                   and D is the embedding dimension.\n","           embed_to_hid_weights, a matrix of size H x ND, where H is the number of hidden units. The first D\n","                   columns represent connections from the embedding of the first context word, the next D columns\n","                   for the second context word, and so on. There are N context words.\n","           hid_bias, a vector of length H\n","           hid_to_output_weights, a matrix of size NV x H\n","           output_bias, a vector of length NV\"\"\"\n","\n","    def __init__(self, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n","                 hid_bias, output_bias):\n","        self.word_embedding_weights = word_embedding_weights\n","        self.embed_to_hid_weights = embed_to_hid_weights\n","        self.hid_to_output_weights = hid_to_output_weights\n","        self.hid_bias = hid_bias\n","        self.output_bias = output_bias\n","\n","    def copy(self):\n","        return self.__class__(self.word_embedding_weights.copy(), self.embed_to_hid_weights.copy(),\n","                              self.hid_to_output_weights.copy(), self.hid_bias.copy(), self.output_bias.copy())\n","\n","    @classmethod\n","    def zeros(cls, vocab_size, context_len, embedding_dim, num_hid):\n","        \"\"\"A constructor which initializes all weights and biases to 0.\"\"\"\n","        word_embedding_weights = np.zeros((vocab_size, embedding_dim))\n","        embed_to_hid_weights = np.zeros((num_hid, context_len * embedding_dim))\n","        hid_to_output_weights = np.zeros((vocab_size * context_len, num_hid))\n","        hid_bias = np.zeros(num_hid)\n","        output_bias = np.zeros(vocab_size * context_len)\n","        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n","                   hid_bias, output_bias)\n","\n","    @classmethod\n","    def random_init(cls, init_wt, vocab_size, context_len, embedding_dim, num_hid):\n","        \"\"\"A constructor which initializes weights to small random values and biases to 0.\"\"\"\n","        word_embedding_weights = np.random.normal(0., init_wt, size=(vocab_size, embedding_dim))\n","        embed_to_hid_weights = np.random.normal(0., init_wt, size=(num_hid, context_len * embedding_dim))\n","        hid_to_output_weights = np.random.normal(0., init_wt, size=(vocab_size * context_len, num_hid))\n","        hid_bias = np.zeros(num_hid)\n","        output_bias = np.zeros(vocab_size * context_len)\n","        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n","                   hid_bias, output_bias)\n","\n","    ###### The functions below are Python's somewhat oddball way of overloading operators, so that\n","    ###### we can do arithmetic on Params instances. You don't need to understand this to do the assignment.\n","\n","    def __mul__(self, a):\n","        return self.__class__(a * self.word_embedding_weights,\n","                              a * self.embed_to_hid_weights,\n","                              a * self.hid_to_output_weights,\n","                              a * self.hid_bias,\n","                              a * self.output_bias)\n","\n","    def __rmul__(self, a):\n","        return self * a\n","\n","    def __add__(self, other):\n","        return self.__class__(self.word_embedding_weights + other.word_embedding_weights,\n","                              self.embed_to_hid_weights + other.embed_to_hid_weights,\n","                              self.hid_to_output_weights + other.hid_to_output_weights,\n","                              self.hid_bias + other.hid_bias,\n","                              self.output_bias + other.output_bias)\n","\n","    def __sub__(self, other):\n","        return self + -1. * other"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6XFQUPsLSi7"},"outputs":[],"source":["class Activations(object):\n","    \"\"\"A class representing the activations of the units in the network. This class has three fields:\n","\n","        embedding_layer, a matrix of B x ND matrix (where B is the batch size, D is the embedding dimension,\n","                and N is the number of input context words), representing the activations for the embedding\n","                layer on all the cases in a batch. The first D columns represent the embeddings for the\n","                first context word, and so on.\n","        hidden_layer, a B x H matrix representing the hidden layer activations for a batch\n","        output_layer, a B x V matrix representing the output layer activations for a batch\"\"\"\n","\n","    def __init__(self, embedding_layer, hidden_layer, output_layer):\n","        self.embedding_layer = embedding_layer\n","        self.hidden_layer = hidden_layer\n","        self.output_layer = output_layer\n","\n","def get_batches(inputs, batch_size, shuffle=True):\n","    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n","    'generator', i.e. something you can use in a for loop. You don't need to understand how it\n","    works to do the assignment.\"\"\"\n","\n","    if inputs.shape[0] % batch_size != 0:\n","        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n","    num_batches = inputs.shape[0] // batch_size\n","\n","    if shuffle:\n","        idxs = np.random.permutation(inputs.shape[0])\n","        inputs = inputs[idxs, :]\n","\n","    for m in range(num_batches):\n","        yield inputs[m * batch_size:(m + 1) * batch_size, :]"]},{"cell_type":"markdown","metadata":{"id":"uuAXaDNll0lf"},"source":["In this part of the assignment, you implement a method which computes the gradient using backpropagation.\n","To start you out, the *Model* class contains several important methods used in training:\n","\n","\n","*   `compute_activations` computes the activations of all units on a given input batch\n","*   `compute_loss` computes the total cross-entropy loss on a mini-batch\n","*   `evaluate` computes the average cross-entropy loss for a given set of inputs and targets\n","\n","You will need to complete the implementation of two additional methods which are needed for training, and print the outputs of the gradients."]},{"cell_type":"markdown","metadata":{"id":"lVF5TxDgtqHx"},"source":[" ## 2.1 Implement gradient with respect to output layer inputs [8pts]\n","* [ ] **TO BE IMPLEMENTED**: `compute_loss_derivative` computes the derivative of the loss function with respect to the output layer inputs.\n","\n","In other words, if $C$ is the cost function, and the softmax computation for the $j$-th word in vocabulary for the $n$-th output word position is:\n","\n","$$y_{n,j} = \\frac{e^{z_{n,j}}}{\\sum_{l} e^{z_{n,l   }}}$$\n","\n","This function should compute a $B \\times NV$ matrix where the entries correspond to the partial derivatives $\\partial C / \\partial z_{j}^{n}$. Recall that the output units are concatenated across all positions, i.e. the $(j + nV)$-th column is for the word $j$ in vocabulary for the $n$-th output word position.\n","\n","## 2.2 Implement gradient with respect to parameters [8pts]\n","* [ ] **TO BE IMPLEMENTED**: `back_propagate` is the function which computes the gradient of the loss with respect to model parameters using backpropagation.\n","It uses the derivatives computed by *compute_loss_derivative*.\n","Some parts are already filled in for you, but you need to compute the matrices of derivatives for `embed_to_hid_weights`, `hid_bias`, `hid_to_output_weights`, and `output_bias`.\n","These matrices have the same sizes as the parameter matrices (see previous section).\n","\n","In order to implement backpropagation efficiently, you need to express the computations in terms of matrix operations, rather than *for* loops.\n","You should first work through the derivatives on pencil and paper.\n","First, apply the chain rule to compute the derivatives with respect to individual units, weights, and biases.\n","Next, take the formulas you've derived, and express them in matrix form.\n","You should be able to express all of the required computations using only matrix multiplication, matrix transpose, and elementwise operations --- no *for* loops!\n","If you want inspiration, read through the code for *Model.compute_activations* and try to understand how the matrix operations correspond to the computations performed by all the units in the network.\n","        \n","To make your life easier, we have provided the routine `checking.check_gradients`, which checks your gradients using finite differences.\n","You should make sure this check passes before continuing with the assignment.\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0F4CTBipK9B6"},"outputs":[],"source":["class Model(object):\n","    \"\"\"A class representing the language model itself. This class contains various methods used in training\n","    the model and visualizing the learned representations. It has two fields:\n","\n","        params, a Params instance which contains the model parameters\n","        vocab, a list containing all the words in the dictionary; vocab[0] is the word with index\n","               0, and so on.\"\"\"\n","\n","    def __init__(self, params, vocab):\n","        self.params = params\n","        self.vocab = vocab\n","\n","        self.vocab_size = len(vocab)\n","        self.embedding_dim = self.params.word_embedding_weights.shape[1]\n","        self.embedding_layer_dim = self.params.embed_to_hid_weights.shape[1]\n","        self.context_len = self.embedding_layer_dim // self.embedding_dim\n","        self.num_hid = self.params.embed_to_hid_weights.shape[0]\n","\n","    def copy(self):\n","        return self.__class__(self.params.copy(), self.vocab[:])\n","\n","    @classmethod\n","    def random_init(cls, init_wt, vocab, context_len, embedding_dim, num_hid):\n","        \"\"\"Constructor which randomly initializes the weights to Gaussians with standard deviation init_wt\n","        and initializes the biases to all zeros.\"\"\"\n","        params = Params.random_init(init_wt, len(vocab), context_len, embedding_dim, num_hid)\n","        return Model(params, vocab)\n","\n","    def indicator_matrix(self, targets, mask_zero_index=True):\n","        \"\"\"Construct a matrix where the (k + j*V)th entry of row i is 1 if the j-th target word\n","         for example i is k, and all other entries are 0.\n","\n","         Note: if the j-th target word index is 0, this corresponds to the [MASK] token,\n","               and we set the entry to be 0.\n","        \"\"\"\n","        batch_size, context_len = targets.shape\n","        expanded_targets = np.zeros((batch_size, context_len * len(self.vocab)))\n","        targets_offset = np.repeat((np.arange(context_len) * len(self.vocab))[np.newaxis, :], batch_size, axis=0) # [[0, V, 2V], [0, V, 2V], ...]\n","        targets += targets_offset\n","\n","        for c in range(context_len):\n","          expanded_targets[np.arange(batch_size), targets[:,c]] = 1.\n","          if mask_zero_index:\n","            # Note: Set the targets with index 0, V, 2V to be zero since it corresponds to the [MASK] token\n","            expanded_targets[np.arange(batch_size), targets_offset[:,c]] = 0.\n","        return expanded_targets\n","\n","    def compute_loss_derivative(self, output_activations, expanded_target_batch, target_mask):\n","        \"\"\"Compute the derivative of the multiple target position cross-entropy loss function \\n\"\n","\n","            For example:\n","\n","         [y_{0} ....  y_{V-1}] [y_{V}, ..., y_{2*V-1}] [y_{2*V} ... y_{i,3*V-1}] [y_{3*V} ... y_{i,4*V-1}]\n","\n","         Where for colum j + n*V,\n","\n","            y_{j + n*V} = e^{z_{j + n*V}} / \\sum_{m=0}^{V-1} e^{z_{m + n*V}}, for n=0,...,N-1\n","\n","        This function should return a dC / dz matrix of size [batch_size x (vocab_size * context_len)],\n","        where each row i in dC / dz has columns 0 to V-1 containing the gradient the 1st output\n","        context word from i-th training example, then columns vocab_size to 2*vocab_size - 1 for the 2nd\n","        output context word of the i-th training example, etc.\n","\n","        C is the loss function summed acrossed all examples as well:\n","\n","            C = -\\sum_{i,j,n} mask_{i,n} (t_{i, j + n*V} log y_{i, j + n*V}), for j=0,...,V, and n=0,...,N\n","\n","        where mask_{i,n} = 1 if the i-th training example has n-th context word as the target,\n","        otherwise mask_{i,n} = 0.\n","\n","        The arguments are as follows:\n","\n","            output_activations - A [batch_size x (context_len * vocab_size)] tensor,\n","                for the activations of the output layer, i.e. the y_j's.\n","            expanded_target_batch - A [batch_size (context_len * vocab_size)] tensor,\n","                where expanded_target_batch[i,n*V:(n+1)*V] is the indicator vector for\n","                the n-th context target word position, i.e. the (i, j + n*V) entry is 1 if the\n","                i'th example, the context word at position n is j, and 0 otherwise.\n","            target_mask - A [batch_size x context_len x 1] tensor, where target_mask[i,n] = 1\n","                if for the i'th example the n-th context word is a target position, otherwise 0\n","\n","        Outputs:\n","            loss_derivative - A [batch_size x (context_len * vocab_size)] matrix,\n","                where loss_derivative[i,0:vocab_size] contains the gradient\n","                dC / dz_0 for the i-th training example gradient for 1st output\n","                context word, and loss_derivative[i,vocab_size:2*vocab_size] for\n","                the 2nd output context word of the i-th training example, etc.\n","        \"\"\"\n","\n","        ###########################   YOUR CODE HERE  ##############################\n","        pass\n","        ############################################################################\n","\n","    def compute_loss(self, output_activations, expanded_target_batch):\n","        \"\"\"Compute the total loss over a mini-batch. expanded_target_batch is the matrix obtained\n","        by calling indicator_matrix on the targets for the batch.\"\"\"\n","        return -np.sum(expanded_target_batch * np.log(output_activations + TINY))\n","\n","    def compute_activations(self, inputs):\n","        \"\"\"Compute the activations on a batch given the inputs. Returns an Activations instance.\n","        You should try to read and understand this function, since this will give you clues for\n","        how to implement back_propagate.\"\"\"\n","\n","        batch_size = inputs.shape[0]\n","        if inputs.shape[1] != self.context_len:\n","            raise RuntimeError('Dimension of the input vectors should be {}, but is instead {}'.format(\n","                self.context_len, inputs.shape[1]))\n","\n","        # Embedding layer\n","        # Look up the input word indies in the word_embedding_weights matrix\n","        embedding_layer_state = np.zeros((batch_size, self.embedding_layer_dim))\n","        for i in range(self.context_len):\n","            embedding_layer_state[:, i * self.embedding_dim:(i + 1) * self.embedding_dim] = \\\n","                self.params.word_embedding_weights[inputs[:, i], :]\n","\n","        # Hidden layer\n","        inputs_to_hid = np.dot(embedding_layer_state, self.params.embed_to_hid_weights.T) + \\\n","                        self.params.hid_bias\n","        # Apply logistic activation function\n","        hidden_layer_state = 1. / (1. + np.exp(-inputs_to_hid))\n","\n","        # Output layer\n","        inputs_to_softmax = np.dot(hidden_layer_state, self.params.hid_to_output_weights.T) + \\\n","                            self.params.output_bias\n","\n","        # Subtract maximum.\n","        # Remember that adding or subtracting the same constant from each input to a\n","        # softmax unit does not affect the outputs. So subtract the maximum to\n","        # make all inputs <= 0. This prevents overflows when computing their exponents.\n","        inputs_to_softmax -= inputs_to_softmax.max(1).reshape((-1, 1))\n","\n","        # Take softmax along each V chunks in the output layer\n","        output_layer_state = np.exp(inputs_to_softmax)\n","        output_layer_state_shape = output_layer_state.shape\n","        output_layer_state = output_layer_state.reshape((-1, self.context_len, len(self.vocab)))\n","        output_layer_state /= output_layer_state.sum(axis=-1, keepdims=True) # Softmax along each target word\n","        output_layer_state = output_layer_state.reshape(output_layer_state_shape) # Flatten back\n","\n","        return Activations(embedding_layer_state, hidden_layer_state, output_layer_state)\n","\n","    def back_propagate(self, input_batch, activations, loss_derivative):\n","        \"\"\"Compute the gradient of the loss function with respect to the trainable parameters\n","        of the model. The arguments are as follows:\n","\n","             input_batch - the indices of the context words\n","             activations - an Activations class representing the output of Model.compute_activations\n","             loss_derivative - the matrix of derivatives computed by compute_loss_derivative\n","\n","        Part of this function is already completed, but you need to fill in the derivative\n","        computations for hid_to_output_weights_grad, output_bias_grad, embed_to_hid_weights_grad,\n","        and hid_bias_grad. See the documentation for the Params class for a description of what\n","        these matrices represent.\"\"\"\n","\n","        # The matrix with values dC / dz_j, where dz_j is the input to the jth hidden unit,\n","        # i.e. h_j = 1 / (1 + e^{-z_j})\n","        hid_deriv = np.dot(loss_derivative, self.params.hid_to_output_weights) \\\n","                    * activations.hidden_layer * (1. - activations.hidden_layer)\n","\n","        ###########################   YOUR CODE HERE  ##############################\n","        # hid_to_output_weights_grad = ...\n","        # output_bias_grad = ...\n","        # embed_to_hid_weights_grad = ...\n","        # hid_bias_grad = ...\n","        ############################################################################\n","\n","        # The matrix of derivatives for the embedding layer\n","        embed_deriv = np.dot(hid_deriv, self.params.embed_to_hid_weights)\n","\n","        # Embedding layer\n","        word_embedding_weights_grad = np.zeros((self.vocab_size, self.embedding_dim))\n","        for w in range(self.context_len):\n","            word_embedding_weights_grad += np.dot(self.indicator_matrix(input_batch[:, w:w+1], mask_zero_index=False).T,\n","                                                  embed_deriv[:, w * self.embedding_dim:(w + 1) * self.embedding_dim])\n","\n","        return Params(word_embedding_weights_grad, embed_to_hid_weights_grad, hid_to_output_weights_grad,\n","                      hid_bias_grad, output_bias_grad)\n","\n","    def sample_input_mask(self, batch_size):\n","        \"\"\"Samples a binary mask for the inputs of size batch_size x context_len\n","        For each row, at most one element will be 1.\n","        \"\"\"\n","        mask_idx = np.random.randint(self.context_len, size=(batch_size,))\n","        mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n","        mask[np.arange(batch_size), mask_idx] = 1\n","        return mask\n","\n","    def evaluate(self, inputs, batch_size=100):\n","        \"\"\"Compute the average cross-entropy over a dataset.\n","\n","            inputs: matrix of shape D x N\"\"\"\n","\n","        ndata = inputs.shape[0]\n","\n","        total = 0.\n","        for input_batch in get_batches(inputs, batch_size):\n","            mask = self.sample_input_mask(batch_size)\n","            input_batch_masked = input_batch * (1 - mask)\n","            activations = self.compute_activations(input_batch_masked)\n","            target_batch_masked = input_batch * mask\n","            expanded_target_batch = self.indicator_matrix(target_batch_masked)\n","            cross_entropy = -np.sum(expanded_target_batch * np.log(activations.output_layer + TINY))\n","            total += cross_entropy\n","\n","        return total / float(ndata)\n","\n","    def display_nearest_words(self, word, k=10):\n","        \"\"\"List the k words nearest to a given word, along with their distances.\"\"\"\n","\n","        if word not in self.vocab:\n","            print('Word \"{}\" not in vocabulary.'.format(word))\n","            return\n","\n","        # Compute distance to every other word.\n","        idx = self.vocab.index(word)\n","        word_rep = self.params.word_embedding_weights[idx, :]\n","        diff = self.params.word_embedding_weights - word_rep.reshape((1, -1))\n","        distance = np.sqrt(np.sum(diff ** 2, axis=1))\n","\n","        # Sort by distance.\n","        order = np.argsort(distance)\n","        order = order[1:1 + k]  # The nearest word is the query word itself, skip that.\n","        for i in order:\n","            print('{}: {}'.format(self.vocab[i], distance[i]))\n","\n","    def word_distance(self, word1, word2):\n","        \"\"\"Compute the distance between the vector representations of two words.\"\"\"\n","\n","        if word1 not in self.vocab:\n","            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n","        if word2 not in self.vocab:\n","            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n","\n","        idx1, idx2 = self.vocab.index(word1), self.vocab.index(word2)\n","        word_rep1 = self.params.word_embedding_weights[idx1, :]\n","        word_rep2 = self.params.word_embedding_weights[idx2, :]\n","        diff = word_rep1 - word_rep2\n","        return np.sqrt(np.sum(diff ** 2))"]},{"cell_type":"markdown","metadata":{"id":"JbwZCTkboEhz"},"source":["## 2.3 Print the gradients [8pts]\n","\n","To make your life easier, we have provided the routine `check_gradients`, which checks your gradients using finite differences.\n","        You should make sure this check passes before continuing with the assignment. Once `check_gradients()` passes, call `print_gradients()` and include its output in your write-up."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5soRTiRn6W4"},"outputs":[],"source":["def relative_error(a, b):\n","    return np.abs(a - b) / (np.abs(a) + np.abs(b))\n","\n","\n","def check_output_derivatives(model, input_batch, target_batch):\n","    def softmax(z):\n","        z = z.copy()\n","        z -= z.max(-1, keepdims=True)\n","        y = np.exp(z)\n","        y /= y.sum(-1, keepdims=True)\n","        return y\n","\n","    batch_size = input_batch.shape[0]\n","    z = np.random.normal(size=(batch_size, model.context_len, model.vocab_size))\n","    y = softmax(z).reshape((batch_size, model.context_len * model.vocab_size))\n","    z = z.reshape((batch_size, model.context_len * model.vocab_size))\n","\n","    expanded_target_batch = model.indicator_matrix(target_batch)\n","    target_mask = expanded_target_batch.reshape(-1, model.context_len, len(model.vocab)).sum(axis=-1, keepdims=True)\n","    loss_derivative = model.compute_loss_derivative(y, expanded_target_batch, target_mask)\n","\n","    if loss_derivative is None:\n","        print('Loss derivative not implemented yet.')\n","        return False\n","\n","    if loss_derivative.shape != (batch_size, model.vocab_size * model.context_len):\n","        print('Loss derivative should be size {} but is actually {}.'.format(\n","            (batch_size, model.vocab_size), loss_derivative.shape))\n","        return False\n","\n","    def obj(z):\n","        z = z.reshape((-1, model.context_len, model.vocab_size))\n","        y = softmax(z).reshape((batch_size, model.context_len * model.vocab_size))\n","        return model.compute_loss(y, expanded_target_batch)\n","\n","    for count in range(1000):\n","        i, j = np.random.randint(0, loss_derivative.shape[0]), np.random.randint(0, loss_derivative.shape[1])\n","\n","        z_plus = z.copy()\n","        z_plus[i, j] += EPS\n","        obj_plus = obj(z_plus)\n","\n","        z_minus = z.copy()\n","        z_minus[i, j] -= EPS\n","        obj_minus = obj(z_minus)\n","\n","        empirical = (obj_plus - obj_minus) / (2. * EPS)\n","        rel = relative_error(empirical, loss_derivative[i, j])\n","        if rel > 1e-4:\n","            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n","            return False\n","\n","    print('The loss derivative looks OK.')\n","    return True\n","\n","\n","def check_param_gradient(model, param_name, input_batch, target_batch):\n","    activations = model.compute_activations(input_batch)\n","    expanded_target_batch = model.indicator_matrix(target_batch)\n","    target_mask = expanded_target_batch.reshape(-1, model.context_len, len(model.vocab)).sum(axis=-1, keepdims=True)\n","    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, target_mask)\n","    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n","\n","    def obj(model):\n","        activations = model.compute_activations(input_batch)\n","        return model.compute_loss(activations.output_layer, expanded_target_batch)\n","\n","    dims = getattr(model.params, param_name).shape\n","    is_matrix = (len(dims) == 2)\n","\n","    if getattr(param_gradient, param_name).shape != dims:\n","        print('The gradient for {} should be size {} but is actually {}.'.format(\n","            param_name, dims, getattr(param_gradient, param_name).shape))\n","        return\n","\n","    for count in range(1000):\n","        if is_matrix:\n","            slc = np.random.randint(0, dims[0]), np.random.randint(0, dims[1])\n","        else:\n","            slc = np.random.randint(dims[0])\n","\n","        model_plus = model.copy()\n","        getattr(model_plus.params, param_name)[slc] += EPS\n","        obj_plus = obj(model_plus)\n","\n","        model_minus = model.copy()\n","        getattr(model_minus.params, param_name)[slc] -= EPS\n","        obj_minus = obj(model_minus)\n","\n","        empirical = (obj_plus - obj_minus) / (2. * EPS)\n","        exact = getattr(param_gradient, param_name)[slc]\n","        rel = relative_error(empirical, exact)\n","        if rel > 3e-4:\n","            import pdb; pdb.set_trace()\n","            print('The loss derivative has a relative error of {}, which is too large for param {}.'.format(rel, param_name))\n","            return False\n","\n","    print('The gradient for {} looks OK.'.format(param_name))\n","\n","\n","def load_partially_trained_model():\n","    obj = pickle.load(open(PARTIALLY_TRAINED_MODEL, 'rb'))\n","    params = Params(obj['word_embedding_weights'], obj['embed_to_hid_weights'],\n","                                   obj['hid_to_output_weights'], obj['hid_bias'],\n","                                   obj['output_bias'])\n","    vocab = obj['vocab']\n","    return Model(params, vocab)\n","\n","\n","def check_gradients():\n","    \"\"\"Check the computed gradients using finite differences.\"\"\"\n","    np.random.seed(0)\n","\n","    np.seterr(all='ignore')  # suppress a warning which is harmless\n","\n","    model = load_partially_trained_model()\n","    data_obj = pickle.load(open(data_location, 'rb'))\n","    train_inputs = data_obj['train_inputs']\n","    input_batch = train_inputs[:100, :]\n","    mask = model.sample_input_mask(input_batch.shape[0])\n","    input_batch_masked = input_batch * (1 - mask)\n","    target_batch_masked = input_batch * mask\n","\n","    if not check_output_derivatives(model, input_batch_masked, target_batch_masked):\n","        return\n","\n","    for param_name in ['word_embedding_weights', 'embed_to_hid_weights', 'hid_to_output_weights',\n","                       'hid_bias', 'output_bias']:\n","        input_batch_masked = input_batch * (1 - mask)\n","        target_batch_masked = input_batch * mask\n","        check_param_gradient(model, param_name, input_batch_masked, target_batch_masked)\n","\n","\n","def print_gradients():\n","    \"\"\"Print out certain derivatives for grading.\"\"\"\n","    np.random.seed(0)\n","\n","    model = load_partially_trained_model()\n","    data_obj = pickle.load(open(data_location, 'rb'))\n","    train_inputs = data_obj['train_inputs']\n","    input_batch = train_inputs[:100, :]\n","\n","    mask = model.sample_input_mask(input_batch.shape[0])\n","    input_batch_masked = input_batch * (1 - mask)\n","    activations = model.compute_activations(input_batch_masked)\n","    target_batch_masked = input_batch * mask\n","    expanded_target_batch = model.indicator_matrix(target_batch_masked)\n","    target_mask = expanded_target_batch.reshape(-1, model.context_len, len(model.vocab)).sum(axis=-1, keepdims=True)\n","    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, target_mask)\n","    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n","\n","    print('loss_derivative[2, 5]', loss_derivative[2, 5])\n","    print('loss_derivative[2, 121]', loss_derivative[2, 121])\n","    print('loss_derivative[5, 33]', loss_derivative[5, 33])\n","    print('loss_derivative[5, 31]', loss_derivative[5, 31])\n","    print()\n","    print('param_gradient.word_embedding_weights[27, 2]', param_gradient.word_embedding_weights[27, 2])\n","    print('param_gradient.word_embedding_weights[43, 3]', param_gradient.word_embedding_weights[43, 3])\n","    print('param_gradient.word_embedding_weights[22, 4]', param_gradient.word_embedding_weights[22, 4])\n","    print('param_gradient.word_embedding_weights[2, 5]', param_gradient.word_embedding_weights[2, 5])\n","    print()\n","    print('param_gradient.embed_to_hid_weights[10, 2]', param_gradient.embed_to_hid_weights[10, 2])\n","    print('param_gradient.embed_to_hid_weights[15, 3]', param_gradient.embed_to_hid_weights[15, 3])\n","    print('param_gradient.embed_to_hid_weights[30, 9]', param_gradient.embed_to_hid_weights[30, 9])\n","    print('param_gradient.embed_to_hid_weights[35, 21]', param_gradient.embed_to_hid_weights[35, 21])\n","    print()\n","    print('param_gradient.hid_bias[10]', param_gradient.hid_bias[10])\n","    print('param_gradient.hid_bias[20]', param_gradient.hid_bias[20])\n","    print()\n","    print('param_gradient.output_bias[0]', param_gradient.output_bias[0])\n","    print('param_gradient.output_bias[1]', param_gradient.output_bias[1])\n","    print('param_gradient.output_bias[2]', param_gradient.output_bias[2])\n","    print('param_gradient.output_bias[3]', param_gradient.output_bias[3])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Tlficab3ZfJ"},"outputs":[],"source":["# Run this to check if your implement gradients matches the finite difference within tolerance\n","# Note: this may take a few minutes to go through all the checks\n","check_gradients()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TCLl7v189SI"},"outputs":[],"source":["# Run this to print out the gradients\n","print_gradients()"]},{"cell_type":"markdown","metadata":{"id":"qtC-br-N5xGT"},"source":["## 2.4 Run model training [2pts]\n","\n","Once you've implemented the gradient computation, you'll need to train the model.\n","The function *train* implements the main training procedure.\n","It takes two arguments:\n","\n","\n","*   `embedding_dim`: The number of dimensions in the distributed representation.\n","*   `num_hid`: The number of hidden units\n","\n","\n","As the model trains, the script prints out some numbers that tell you how well the training is going.\n","It shows:\n","\n","\n","*   The cross entropy on the last 100 mini-batches of the training set. This is shown after every 100 mini-batches.\n","*   The cross entropy on the entire validation set every 1000 mini-batches of training.\n","\n","At the end of training, this function shows the cross entropies on the training, validation and test sets.\n","It will return a *Model* instance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akBYJQOdLfaF"},"outputs":[],"source":["_train_inputs = None\n","_train_targets = None\n","_vocab = None\n","\n","DEFAULT_TRAINING_CONFIG = {'batch_size': 100,  # the size of a mini-batch\n","                           'learning_rate': 0.1,  # the learning rate\n","                           'momentum': 0.9,  # the decay parameter for the momentum vector\n","                           'epochs': 50,  # the maximum number of epochs to run\n","                           'init_wt': 0.01,  # the standard deviation of the initial random weights\n","                           'context_len': 4,  # the number of context words used\n","                           'show_training_CE_after': 100,  # measure training error after this many mini-batches\n","                           'show_validation_CE_after': 1000,  # measure validation error after this many mini-batches\n","                           }\n","\n","\n","def find_occurrences(word1, word2, word3):\n","    \"\"\"Lists all the words that followed a given tri-gram in the training set and the number of\n","    times each one followed it.\"\"\"\n","\n","    # cache the data so we don't keep reloading\n","    global _train_inputs, _train_targets, _vocab\n","    if _train_inputs is None:\n","        data_obj = pickle.load(open(data_location, 'rb'))\n","        _vocab = data_obj['vocab']\n","        _train_inputs, _train_targets = data_obj['train_inputs'], data_obj['train_targets']\n","\n","    if word1 not in _vocab:\n","        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n","    if word2 not in _vocab:\n","        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n","    if word3 not in _vocab:\n","        raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word3))\n","\n","    idx1, idx2, idx3 = _vocab.index(word1), _vocab.index(word2), _vocab.index(word3)\n","    idxs = np.array([idx1, idx2, idx3])\n","\n","    matches = np.all(_train_inputs == idxs.reshape((1, -1)), 1)\n","\n","    if np.any(matches):\n","        counts = collections.defaultdict(int)\n","        for m in np.where(matches)[0]:\n","            counts[_vocab[_train_targets[m]]] += 1\n","\n","        word_counts = sorted(list(counts.items()), key=lambda t: t[1], reverse=True)\n","        print('The tri-gram \"{} {} {}\" was followed by the following words in the training set:'.format(\n","            word1, word2, word3))\n","        for word, count in word_counts:\n","            if count > 1:\n","                print('    {} ({} times)'.format(word, count))\n","            else:\n","                print('    {} (1 time)'.format(word))\n","    else:\n","        print('The tri-gram \"{} {} {}\" did not occur in the training set.'.format(word1, word2, word3))\n","\n","\n","def train(embedding_dim, num_hid, config=DEFAULT_TRAINING_CONFIG):\n","    \"\"\"This is the main training routine for the language model. It takes two parameters:\n","\n","        embedding_dim, the dimension of the embedding space\n","        num_hid, the number of hidden units.\"\"\"\n","    # For reproducibility\n","    np.random.seed(123)\n","\n","    # Load the data\n","    data_obj = pickle.load(open(data_location, 'rb'))\n","    vocab = data_obj['vocab']\n","    train_inputs = data_obj['train_inputs']\n","    valid_inputs = data_obj['valid_inputs']\n","    test_inputs = data_obj['test_inputs']\n","\n","    # Randomly initialize the trainable parameters\n","    model = Model.random_init(config['init_wt'], vocab, config['context_len'], embedding_dim, num_hid)\n","\n","    # Variables used for early stopping\n","    best_valid_CE = np.infty\n","    end_training = False\n","\n","    # Initialize the momentum vector to all zeros\n","    delta = Params.zeros(len(vocab), config['context_len'], embedding_dim, num_hid)\n","\n","    this_chunk_CE = 0.\n","    batch_count = 0\n","    for epoch in range(1, config['epochs'] + 1):\n","        if end_training:\n","            break\n","\n","        print()\n","        print('Epoch', epoch)\n","\n","        for m, (input_batch) in enumerate(get_batches(train_inputs, config['batch_size'])):\n","            batch_count += 1\n","\n","            # For each example (row in input_batch), select one word to mask out\n","            mask = model.sample_input_mask(config['batch_size'])\n","            input_batch_masked = input_batch * (1 - mask) # We only zero out one word per row\n","            target_batch_masked = input_batch * mask # We want to predict the masked out word\n","\n","            # Forward propagate\n","            activations = model.compute_activations(input_batch_masked)\n","\n","            # Compute loss derivative\n","            expanded_target_batch = model.indicator_matrix(target_batch_masked)\n","            loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, mask[:,:, np.newaxis])\n","            loss_derivative /= config['batch_size']\n","\n","            # Measure loss function\n","            cross_entropy = model.compute_loss(activations.output_layer, expanded_target_batch) / config['batch_size']\n","            this_chunk_CE += cross_entropy\n","            if batch_count % config['show_training_CE_after'] == 0:\n","                print('Batch {} Train CE {:1.3f}'.format(\n","                    batch_count, this_chunk_CE / config['show_training_CE_after']))\n","                this_chunk_CE = 0.\n","\n","            # Backpropagate\n","            loss_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n","\n","            # Update the momentum vector and model parameters\n","            delta = config['momentum'] * delta + loss_gradient\n","            model.params -= config['learning_rate'] * delta\n","\n","            # Validate\n","            if batch_count % config['show_validation_CE_after'] == 0:\n","                print('Running validation...')\n","                cross_entropy = model.evaluate(valid_inputs)\n","                print('Validation cross-entropy: {:1.3f}'.format(cross_entropy))\n","\n","                if cross_entropy > best_valid_CE:\n","                    print('Validation error increasing!  Training stopped.')\n","                    end_training = True\n","                    break\n","\n","                best_valid_CE = cross_entropy\n","\n","    print()\n","    train_CE = model.evaluate(train_inputs)\n","    print('Final training cross-entropy: {:1.3f}'.format(train_CE))\n","    valid_CE = model.evaluate(valid_inputs)\n","    print('Final validation cross-entropy: {:1.3f}'.format(valid_CE))\n","    test_CE = model.evaluate(test_inputs)\n","    print('Final test cross-entropy: {:1.3f}'.format(test_CE))\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ZX-g3K-F55h7"},"source":["Run the training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwlRG7j8LmIM"},"outputs":[],"source":["embedding_dim = 16\n","num_hid = 128\n","trained_model = train(embedding_dim, num_hid)"]},{"cell_type":"markdown","metadata":{"id":"2FD5Om0ypNPe"},"source":["To convince us that you have correctly implemented the gradient computations, please include the following with your assignment submission:\n","\n","* [ ] You will submit `hw1_code_<YOUR_UNI>.ipynb` through CourseWorks.\n","You do not need to modify any of the code except the parts we asked you to implement.\n","* [ ] In your PDF file, include the output of the function `print_gradients`.\n","This prints out part of the gradients for a partially trained network which we have provided, and we will check them against the correct outputs. **Important:** make sure to give the output of `print_gradients`, **not** `check_gradients`.\n","\n","Since we gave you a gradient checker, you have no excuse for not getting full points on this part.\n"]},{"cell_type":"markdown","metadata":{"id":"z-TThqObiw58"},"source":["# Part 3: Arithmetics and Analysis (8pts)\n","In this part, you will perform arithmetic calculations on the word embeddings learned from previous models and analyze the representation learned by the networks with t-SNE plots.\n","    "]},{"cell_type":"markdown","metadata":{"id":"Ic4VJaRMQLhs"},"source":["## 3.1 t-SNE\n","\n","You will first train the models discussed in the previous sections; youâ€™ll use the trained models for the remainding of this section.\n","\n","**Important**: if youâ€™ve made any fixes to your gradient code, you must reload the a1-code module and then re-run the training procedure.\n","Python does not reload modules automatically, and you donâ€™t want to accidentally\n","analyze an old version of your model.\n","\n","These methods of the Model class can be used for analyzing the model after the training is\n","done:\n","* `tsne_plot_representation` creates a 2-dimensional embedding of the distributed representation space using an algorithm called t-SNE. (You donâ€™t need to know what this is for the assignment, but we\n","may cover it later in the course.) Nearby points in this 2-D space are meant to correspond to\n","nearby points in the 16-D space.\n","*   `display_nearest_words` lists the words whose embedding vectors are nearest to the given\n","word\n","*   `word_distance` computes the distance between the embeddings of two words\n","\n","Plot the 2-dimensional visualization for the trained model from part 3 using the method `tsne_plot_representation`.\n","            Look at the plot and find a few clusters of related words.\n","            What do the words in each cluster have in common?\n","            Plot the 2-dimensional visualization for the GloVe model from part 1 using the method `tsne_plot_GLoVe_representation`.\n","            How do the t-SNE embeddings for both models compare?\n","            Plot the 2-dimensional visualization using the method `plot_2d_GLoVe_representation`.\n","            How does this compare to the t-SNE embeddings?\n","            Please answer in 2 sentences for each question and show the plots in your submission."]},{"cell_type":"markdown","metadata":{"id":"_CfHlyJlDw1B"},"source":["3.1 **Answer**:\n","**\\*\\*TODO: Write Part 3.1 answer here\\*\\***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBk5VoiXCPqe"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","def tsne_plot_representation(model):\n","    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n","    print(model.params.word_embedding_weights.shape)\n","    mapped_X = TSNE(n_components=2).fit_transform(model.params.word_embedding_weights)\n","    pylab.figure(figsize=(12,12))\n","    for i, w in enumerate(model.vocab):\n","        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n","    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n","    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n","    pylab.show()\n","\n","def tsne_plot_GLoVE_representation(W_final, b_final):\n","    \"\"\"Plot a 2-D visualization of the learned representations using t-SNE.\"\"\"\n","    mapped_X = TSNE(n_components=2).fit_transform(W_final)\n","    pylab.figure(figsize=(12,12))\n","    data_obj = pickle.load(open(data_location, 'rb'))\n","    for i, w in enumerate(data_obj['vocab']):\n","        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n","    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n","    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n","    pylab.show()\n","\n","def plot_2d_GLoVE_representation(W_final, b_final):\n","    \"\"\"Plot a 2-D visualization of the learned representations.\"\"\"\n","    mapped_X = W_final\n","    pylab.figure(figsize=(12,12))\n","    data_obj = pickle.load(open(data_location, 'rb'))\n","    for i, w in enumerate(data_obj['vocab']):\n","        pylab.text(mapped_X[i, 0], mapped_X[i, 1], w)\n","    pylab.xlim(mapped_X[:, 0].min(), mapped_X[:, 0].max())\n","    pylab.ylim(mapped_X[:, 1].min(), mapped_X[:, 1].max())\n","    pylab.show()"]},{"cell_type":"markdown","metadata":{"id":"Jo2SoHzhCPqf"},"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6mSwi9zCPqf"},"outputs":[],"source":["tsne_plot_representation(trained_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RldUQuJCPqg"},"outputs":[],"source":["tsne_plot_GLoVE_representation(W_final, b_final)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nupzmyYjCPqg"},"outputs":[],"source":["plot_2d_GLoVE_representation(W_final_2d, b_final_2d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUglR9GTCPqg"},"outputs":[],"source":["tsne_plot_GLoVE_representation(W_final_2d, b_final_2d)"]},{"cell_type":"markdown","metadata":{"id":"-DVGkTS3CPqi"},"source":["# What you have to submit\n","\n","For reference, here is everything you need to hand in. See the top of this handout for submission\n","directions.\n","\n","\n","\n","*   Please make sure you finish the solve problems and submit your answers and codes according to the submission instruction:\n","  * [ ] **Part 1**: Questions 1.1, 1.2, 1.3, 1.4. Completed code for `grad_GLoVE` function.\n","  * [ ] **Part 2**: Completed code for `compute_loss_derivative()` (2.1), `back_propagate()` (2.2) functions, and the output of `print_gradients()` (2.3) and (2.4)\n","  * [ ] **Part 3**: Questions 3.1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcVh0VMgFGsW"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["fr75-y9A4zcM"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}